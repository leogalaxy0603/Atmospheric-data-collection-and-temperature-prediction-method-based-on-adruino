%% 初始化
clear
close all
clc
warning off

%% 数据读取
Function_dataname='Dataset1'; % Dateset1=glass1; Dateset2=ecoli3; Dateset3=vehicle3 
                              % Dateset4=newthyroid2; Dateset5=yeast1
data = Get_datasets(Function_dataname);
%  data=xlsread('数据.xlsx','Sheet1','A1:N252'); %%使用xlsread函数读取EXCEL中对应范围的数据即可

%输入输出数据
input=data(:,1:end-1);    %data的第一列-倒数第二列为特征指标
output=data(:,end);  %data的最后面一列为输出的指标值

N=length(output);   %全部样本数目
testNum=300;   %设定测试样本数目 默认15
trainNum=N-testNum;    %计算训练样本数目

%% 划分训练集、测试集
input_train = input(1:trainNum,:)';
output_train =output(1:trainNum)';
input_test =input(trainNum+1:trainNum+testNum,:)';
output_test =output(trainNum+1:trainNum+testNum)';

%% 数据归一化
[inputn,inputps]=mapminmax(input_train,0,1);
[outputn,outputps]=mapminmax(output_train);
inputn_test=mapminmax('apply',input_test,inputps);

%% 获取输入层节点、输出层节点个数
inputnum=size(input,2);
outputnum=size(output,2);
disp('/////////////////////////////////')
disp('神经网络结构...')
disp(['输入层的节点数为：',num2str(inputnum)])
disp(['输出层的节点数为：',num2str(outputnum)])
disp(' ')
disp('隐含层节点的确定过程...')

%确定隐含层节点个数
%采用经验公式hiddennum=sqrt(m+n)+a，m为输入层节点个数，n为输出层节点个数，a一般取为1-10之间的整数
MSE=1e+5; %初始化最小误差
for hiddennum=fix(sqrt(inputnum+outputnum))+1:fix(sqrt(inputnum+outputnum))+10
    
    %构建网络
    net=newelm(inputn,outputn,hiddennum);
    % 网络参数
    net.trainParam.epochs=1000;         % 训练次数
    net.trainParam.lr=0.01;                   % 学习速率
    net.trainParam.goal=0.000001;        % 训练目标最小误差
    % 网络训练
    net=train(net,inputn,outputn);
    an0=sim(net,inputn);  %仿真结果
    mse0=mse(outputn,an0);  %仿真的均方误差
    disp(['隐含层节点数为',num2str(hiddennum),'时，训练集的均方误差为：',num2str(mse0)])
    
    %更新最佳的隐含层节点
    if mse0<MSE
        MSE=mse0;
        hiddennum_best=hiddennum;
    end
end
disp(['最佳的隐含层节点数为：',num2str(hiddennum_best),'，相应的均方误差为：',num2str(MSE)])

%% 构建最佳隐含层节点的ELMAN神经网络
disp(' ')
disp('标准的ELMAN神经网络：')
net0=newelm(inputn,outputn,hiddennum_best,{'tansig','purelin'},'trainlm');% 建立模型

%网络参数配置
net0.trainParam.epochs=1000;         % 训练次数，这里设置为1000次
net0.trainParam.lr=0.01;                   % 学习速率，这里设置为0.01
net0.trainParam.goal=0.00001;                    % 训练目标最小误差，这里设置为0.0001
net0.trainParam.show=25;                % 显示频率，这里设置为每训练25次显示一次
net0.trainParam.mc=0.01;                 % 动量因子
net0.trainParam.min_grad=1e-6;       % 最小性能梯度
net0.trainParam.max_fail=6;               % 最高失败次数

%开始训练
net0=train(net0,inputn,outputn);

%预测
an0=sim(net0,inputn_test); %用训练好的模型进行仿真

%预测结果反归一化与误差计算
test_simu0=mapminmax('reverse',an0,outputps); %把仿真得到的数据还原为原始的数量级
%误差指标
[mae0,mse0,rmse0,mape0,error0,errorPercent0]=calc_error(output_test,test_simu0);

%% 鲸鱼优化算法寻最优权值阈值
disp(' ')
disp('WOA优化ELMAN神经网络：')
net=newelm(inputn,outputn,hiddennum_best,{'tansig','purelin'},'trainlm');% 建立模型

%网络参数配置
net.trainParam.epochs=1000;         % 训练次数，这里设置为1000次
net.trainParam.lr=0.01;                   % 学习速率，这里设置为0.01
net.trainParam.goal=0.00001;                    % 训练目标最小误差，这里设置为0.0001
net.trainParam.show=25;                % 显示频率，这里设置为每训练25次显示一次
net.trainParam.mc=0.01;                 % 动量因子
net.trainParam.min_grad=1e-6;       % 最小性能梯度
net.trainParam.max_fail=6;               % 最高失败次数

%% 初始化WOA参数
popsize=30;   %初始种群规模
maxgen=50;   %最大进化代数
dim=inputnum*hiddennum_best+hiddennum_best*hiddennum_best+hiddennum_best+hiddennum_best*outputnum+outputnum;    %自变量个数
lb=repmat(-3,1,dim);    %自变量下限
ub=repmat(3,1,dim);   %自变量上限
%初始化位置向量和领导者得分
Leader_pos=zeros(1,dim);
Leader_score=10^20;   
Net=net;

%% 初始化种群
for i=1:dim
    ub_i=ub(i);
    lb_i=lb(i);
   Positions(:,i)=rand(popsize,1).*(ub_i-lb_i)+lb_i;
end
curve=zeros(maxgen,1);%初始化收敛曲线
%
% 循环开始
h0 = waitbar(0,'进度','Name','WOA optimization...',...
    'CreateCancelBtn','setappdata(gcbf,''canceling'',1)');
setappdata(h0,'canceling',0);
%}
for t=1:maxgen
    for i=1:size(Positions,1)%对每个个体一个一个检查是否越界
        %对每个个体一个一个检查是否越界
        % 返回超出搜索空间边界的搜索代理
        Flag4ub=Positions(i,:)>ub;
        Flag4lb=Positions(i,:)<lb;
        Positions(i,:)=(Positions(i,:).*(~(Flag4ub+Flag4lb)))+ub.*Flag4ub+lb.*Flag4lb;%超过最大值的设置成最大值，超过最小值的设置成最小值
        %目标函数值的计算
        [fit(i),NET]=fitness(Positions(i,:),inputnum,hiddennum_best,outputnum,net,inputn,outputn,output_train,inputn_test,outputps,output_test);
        
        % 更新领导者位置
        if fit(i)<Leader_score
            Leader_score=fit(i);
            Leader_pos=Positions(i,:);
            Net=NET;
        end
    end
    
    a=2-t*((2)/maxgen);
    a2=-1+t*((-1)/maxgen);
    %参数更新
    for i=1:size(Positions,1)
        r1=rand();r2=rand();
        A=2*a*r1-a;
        C=2*r2;
       
        b=1;
        l=(a2-1)*rand+1;
        
        p = rand();
        
        for j=1:size(Positions,2)%对每一个个体地多维度进行循环运算
            %收缩包围机制
            if p<0.5
                if abs(A)>=1
                    rand_leader_index = floor(popsize*rand()+1);%floor将 X 的每个元素四舍五入到小于或等于该元素的最接近整数
                    X_rand = Positions(rand_leader_index, :);
                    D_X_rand=abs(C*X_rand(j)-Positions(i,j));
                    Positions(i,j)=X_rand(j)-A*D_X_rand;
                elseif abs(A)<1
                    D_Leader=abs(C*Leader_pos(j)-Positions(i,j));
                    Positions(i,j)=Leader_pos(j)-A*D_Leader;
                end
                %螺旋更新位置
            elseif p>=0.5
                distance2Leader=abs(Leader_pos(j)-Positions(i,j));
                Positions(i,j)=distance2Leader*exp(b.*l).*cos(l.*2*pi)+Leader_pos(j);
            end
        end
    end
    curve(t)=Leader_score;
    waitbar(t/maxgen,h0,[num2str(t/maxgen*100),'%'])
    if getappdata(h0,'canceling')
        break
    end
end
delete(h0)

%% 绘制进化曲线
figure
plot(curve,'r-','linewidth',2)
xlabel('进化代数')
ylabel('均方误差')
legend('最佳适应度')
title('WOA进化曲线')
w1=Leader_pos(1:inputnum*hiddennum_best);  %输入层到隐含层的权值元素
w2=Leader_pos(inputnum*hiddennum_best+1:inputnum*hiddennum_best+hiddennum_best*hiddennum_best);  %承接层到隐含层的权值
B1=Leader_pos(inputnum*hiddennum_best+hiddennum_best*hiddennum_best+1:inputnum*hiddennum_best+hiddennum_best*hiddennum_best+hiddennum_best);  %隐含层到输出层的权值元素
w3=Leader_pos(inputnum*hiddennum_best+hiddennum_best*hiddennum_best+hiddennum_best+1:inputnum*hiddennum_best+hiddennum_best*hiddennum_best+hiddennum_best+hiddennum_best*outputnum);  %隐含层的各神经元阈值元素
B2=Leader_pos(inputnum*hiddennum_best+hiddennum_best*hiddennum_best+hiddennum_best+hiddennum_best*outputnum+1:inputnum*hiddennum_best+hiddennum_best*hiddennum_best+hiddennum_best+hiddennum_best*outputnum+outputnum);   %输出层的各神经元阈值元素

%矩阵重构
net.iw{1,1}=reshape(w1,hiddennum_best,inputnum);   %输入层到隐含层的权值
net.lw{1,1}=reshape(w2,hiddennum_best,hiddennum_best);   %承接层到隐含层的权值
net.lw{2,1}=reshape(w3,outputnum,hiddennum_best);   %隐含层到输出层的权值矩阵
net.b{1}=reshape(B1,hiddennum_best,1);    %隐含层的各神经元阈值
net.b{2}=B2;    %输出层的各神经元阈值

%优化后的神经网络
net=Net;

%% 优化后的神经网络测试
an1=sim(net,inputn_test);
test_simu1=mapminmax('reverse',an1,outputps); %把仿真得到的数据还原为原始的数量级
%误差指标
[mae1,mse1,rmse1,mape1,error1,errorPercent1]=calc_error(output_test,test_simu1);


%% 作图
figure
plot(output_test,'b-*','linewidth',1)
hold on
plot(test_simu0,'r-v','linewidth',1,'markerfacecolor','r')
hold on
plot(test_simu1,'k-o','linewidth',1,'markerfacecolor','k')
legend('真实值','ELMAN预测值','WOA-ELMAN预测值')
xlabel('测试样本编号')
ylabel('指标值')
title('WOA优化前后的ELMAN神经网络预测值和真实值对比图')

figure
plot(error0,'rv-','markerfacecolor','r')
hold on
plot(error1,'ko-','markerfacecolor','k')
legend('ELMAN预测误差','WOA-ELMAN预测误差')
xlabel('测试样本编号')
ylabel('预测偏差')
title('WOA优化前后的ELMAN神经网络预测值和真实值误差对比图')

disp(' ')
disp('/////////////////////////////////')
disp('打印结果表格')
disp('样本序号     实测值      ELMAN预测值  WOA-ELMAN值   ELMAN误差   WOA-ELMAN误差')
for i=1:testNum
    disp([i output_test(i),test_simu0(i),test_simu1(i),error0(i),error1(i)])
end

